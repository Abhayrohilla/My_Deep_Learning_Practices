{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f3f7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetLMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "model = XLNetLMHeadModel.from_pretrained('xlnet-large-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5211ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_TEXT = \"\"\"Python is an interpreted high-level general-purpose programming language. Its design philosophy emphasizes code readability with its use of significant indentation. Its language constructs as well as its object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.[30]\n",
    "\n",
    "Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.[31]\n",
    "\n",
    "Guido van Rossum began working on Python in the late 1980s, as a successor to the ABC programming language, and first released it in 1991 as Python 0.9.0.[32] Python 2.0 was released in 2000 and introduced new features, such as list comprehensions and a garbage collection system using reference counting. Python 3.0 was released in 2008 and was a major revision of the language that is not completely backward-compatible. Python 2 was discontinued with version 2.7.18 in 2020.[33]\n",
    "\n",
    "Python consistently ranks as one of the most popular programming languages. <eod> </s> <eos>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe15609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted word: <eod> 0\n",
      "word and logits <eop> -0.9852077960968018\n",
      "word and logits < -4.28443717956543\n",
      "word and logits < -4.850223541259766\n",
      "word and logits By -4.871249675750732\n",
      "word and logits by -5.666018486022949\n",
      "predicted word: < 1\n",
      "word and logits . 8.380627632141113\n",
      "word and logits  8.354589462280273\n",
      "word and logits s 8.143485069274902\n",
      "word and logits / 7.687319278717041\n",
      "word and logits </ 7.284868240356445\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# We show how to setup inputs to predict a next token using a bi-directional context.\n",
    "# We will predict masked tokens\n",
    "input_ids = torch.tensor(tokenizer.encode(PADDING_TEXT, add_special_tokens=False)).unsqueeze(0)  \n",
    "\n",
    "targets = [ -6, -4]\n",
    "\n",
    "perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "perm_mask[0, :, targets] = 1.0  # Previous tokens don't see last token\n",
    "\n",
    "target_mapping = torch.zeros((1, len(targets), input_ids.shape[1]), dtype=torch.float)  \n",
    "\n",
    "target_mapping[0, 0, targets[0]] = 1.0  # Our first  prediction \n",
    "target_mapping[0, 1, targets[1]] = 1.0  # Our second  prediction \n",
    "\n",
    "input_ids_tensor = input_ids.to(\"cpu\")\n",
    "target_mapping_tensor = target_mapping.to(\"cpu\")\n",
    "perm_mask_tensor = perm_mask.to(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
    "next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n",
    "\n",
    "for j in range(len(targets)):\n",
    "  predicted_k_indexes = torch.topk(outputs[0][0][j],k=5)\n",
    "  predicted_logits_list = predicted_k_indexes[0] \n",
    "  predicted_indexes_list = predicted_k_indexes[1] \n",
    "    \n",
    "  print (\"predicted word:\",tokenizer.decode(input_ids[0][targets[j]].item()), j)\n",
    "  for i,item  in enumerate(predicted_indexes_list):\n",
    "      the_index = predicted_indexes_list[i].item()\n",
    "      print(\"word and logits\",tokenizer.decode(the_index),predicted_logits_list[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9b3be9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left:  <eop> python is a\n",
      "right:  <eop> python is a language\n",
      "left:  |<eop> python is a language\n",
      "right:  |<eop> python is a language that\n",
      "left:  /|<eop> python is a language that\n",
      "right:  /|<eop> python is a language that uses\n",
      "left:  /|<eop> python is a language that uses\n",
      "right:  /|<eop> python is a language that uses an\n",
      "left:  <eop> /|<eop> python is a language that uses an\n",
      "right:  <eop> /|<eop> python is a language that uses an interpreted\n",
      "left:  ><eop> /|<eop> python is a language that uses an interpreted\n",
      "right:  ><eop> /|<eop> python is a language that uses an interpreted high\n",
      "left:  br><eop> /|<eop> python is a language that uses an interpreted high\n",
      "right:  br><eop> /|<eop> python is a language that uses an interpreted high\n",
      "left:  /br><eop> /|<eop> python is a language that uses an interpreted high\n",
      "right:  /br><eop> /|<eop> python is a language that uses an interpreted high level\n",
      "left:  /br><eop> /|<eop> python is a language that uses an interpreted high level\n",
      "right:  /br><eop> /|<eop> python is a language that uses an interpreted high level\n",
      "left:  - /br><eop> /|<eop> python is a language that uses an interpreted high level\n",
      "right:  - /br><eop> /|<eop> python is a language that uses an interpreted high level (\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Function to select topK tokens from the probability list and \n",
    "# then based on the selected K word distribution get sample of random token IDs\n",
    "def choose_from_top(probs, k=5, sample_size=1):\n",
    "    ind = np.argpartition(probs, -k)[-k:]\n",
    "    top_prob = probs[ind]\n",
    "    # print(tokenizer.decode(ind))\n",
    "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
    "    choice = np.random.choice(k, sample_size, p = top_prob, replace=False)\n",
    "    token_ids = ind[choice]\n",
    "    return token_ids\n",
    "\n",
    "sent = \"python is a \"\n",
    "topk = 10\n",
    "n = 20\n",
    "# Lower temperatures make the model more confident in its top choices, while temperatures greater than 1 decrease confidence.\n",
    "temperature = 5\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
    "\n",
    "sent_tokens = tokenizer.encode(sent, add_special_tokens=False)\n",
    "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
    "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
    "   \n",
    "for i in range(n):\n",
    "  input = mask_tokens + sent_tokens + mask_tokens     \n",
    "  target_id1 = -len(input)\n",
    "  target_id2 = -1\n",
    "\n",
    "  input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)   # We will predict masked tokens\n",
    "\n",
    "  perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "  perm_mask[0, :, [target_id1, target_id2]] = 1.0  # Previous tokens don't see last token\n",
    "\n",
    "  target_mapping = torch.zeros((1, 2, input_ids.shape[1]), dtype=torch.float)  \n",
    "  target_mapping[0, 0, target_id1] = 1.0  # Our first  prediction \n",
    "  target_mapping[0, 1, target_id2] = 1.0  # Our second  prediction \n",
    "\n",
    "  input_ids_tensor = input_ids.to(\"cpu\")\n",
    "  target_mapping_tensor = target_mapping.to(\"cpu\")\n",
    "  perm_mask_tensor = perm_mask.to(\"cpu\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
    "\n",
    "  predicted_tokens = []\n",
    "  \n",
    "  for j in range(2):\n",
    "    probs = torch.nn.functional.softmax(outputs[0][0][j]/temperature, dim = 0).to('cpu').numpy()\n",
    "    predicted_tokens.append(choose_from_top(probs, k=topk, sample_size=1))\n",
    "\n",
    "  if i % 2 == 0:    \n",
    "    tok = predicted_tokens[0][0]\n",
    "    sent_tokens = [tok] + sent_tokens \n",
    "    print('left: ', tokenizer.decode(sent_tokens))\n",
    "  else:     \n",
    "    tok = predicted_tokens[1][0]\n",
    "    sent_tokens = sent_tokens + [tok]\n",
    "    print(\"right: \", tokenizer.decode(sent_tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37174bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "padding_tokens = tokenizer.encode(PADDING_TEXT, add_special_tokens=False)\n",
    "mask_tokens = tokenizer.encode('<mask>', add_special_tokens=False)\n",
    "\n",
    "model.eval()\n",
    "if torch.cuda.is_available(): model.to('cuda') #if we have a GPU \n",
    "\n",
    "def candidates_gen(sent_tokens, candidate=([], 1, []), d='left', n_candidates=5, topk=20, temperature=5):\n",
    "  branch_candidates = []  \n",
    "  cand_tokens = candidate[0]\n",
    "  \n",
    "  if d == 'right':    \n",
    "    input = sent_tokens + cand_tokens + mask_tokens     \n",
    "    \n",
    "    target_id = -1\n",
    "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
    "\n",
    "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "    perm_mask[0, :, target_id] = 1.0  # Previous tokens don't see last token\n",
    "  else:        \n",
    "    input = mask_tokens + cand_tokens + sent_tokens    \n",
    "    \n",
    "    target_id = -len(input)  \n",
    "    input_ids = torch.tensor(padding_tokens + input).unsqueeze(0)  \n",
    "\n",
    "    perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n",
    "    perm_mask[0, :, [target_id - i for i in range(100)]] = 1.0  # Mask additional previos tokens to improve left-side generation\n",
    "\n",
    "  # We will predict masked tokens \n",
    "  target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  \n",
    "  target_mapping[0, 0, target_id] = 1.0  # Our right  prediction \n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    input_ids_tensor = input_ids.to(\"cuda\")\n",
    "    target_mapping_tensor = target_mapping.to(\"cuda\")\n",
    "    perm_mask_tensor = perm_mask.to(\"cuda\")\n",
    "  else:\n",
    "    input_ids_tensor = input_ids\n",
    "    target_mapping_tensor = target_mapping\n",
    "    perm_mask_tensor = perm_mask\n",
    "\n",
    "  with torch.no_grad():\n",
    "    outputs = model(input_ids_tensor, perm_mask=perm_mask_tensor, target_mapping=target_mapping_tensor)\n",
    "\n",
    "  probs = torch.nn.functional.softmax(outputs[0][0][0]/temperature, dim = 0)\n",
    "  selected_indexes = choose_from_top(probs.to('cpu').numpy(), k=topk, sample_size=n_candidates)\n",
    "  selected_probs = probs[selected_indexes]\n",
    "\n",
    "  for i,item  in enumerate(selected_indexes):\n",
    "      the_index = item.item()\n",
    "      if d == \"right\":\n",
    "        new_sent = cand_tokens + [the_index]\n",
    "      elif d == \"left\":\n",
    "        new_sent = [the_index] + cand_tokens\n",
    "      \n",
    "      prob = selected_probs[i].item()\n",
    "      # add word combinations to branch_candidates in format [sentence, cumulative probability, all probs]\n",
    "      branch_candidates.append((new_sent, candidate[1] * prob, candidate[2] + [prob]))\n",
    "  \n",
    "  return branch_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4dbf039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_gen(sent_tokens, candidates, depth=5, d='right', sample_size=2, topk=10, temperature=5):\n",
    "  beams = candidates[:]\n",
    "  new_candidates = candidates[:]\n",
    "  while depth > 0:\n",
    "    new_candidates = []\n",
    "    for candidate in candidates:\n",
    "      for new_candidate in candidates_gen(sent_tokens, candidate, d, sample_size, topk, temperature):\n",
    "        beams.append(new_candidate)\n",
    "        new_candidates.append(new_candidate)   \n",
    "    print(\"Number of beams:\", len(new_candidates))    \n",
    "    candidates = new_candidates[:]\n",
    "    depth -= 1\n",
    "  # sort candidate beams by a sum of logaryphms of probability of each word in a beam. Which is equivalet to product of probabilities \n",
    "  sorted_beams = sorted(new_candidates, key=lambda tup: np.sum(np.log10(tup[2])), reverse=True)\n",
    "  return beams, sorted_beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1eadc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature):\n",
    "  sent_tokens = tokenizer.encode(sent, add_special_tokens=False) \n",
    "\n",
    "  for i in range(iterations):\n",
    "    if (i % 2 == 0 and direction == 'both') or direction == 'left':\n",
    "      print('>> left side generation')\n",
    "      candidates = candidates_gen(sent_tokens=sent_tokens, d='left', n_candidates=first_sample_size,  topk=topk, temperature=temperature)\n",
    "      beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'left', sample_size, topk, temperature=temperature)\n",
    "      topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
    "      selected_candidate = random.choice(sorted_beams[:topn])\n",
    "      sent_tokens = selected_candidate[0] + sent_tokens\n",
    "      print(tokenizer.decode(sent_tokens))\n",
    "    if (i % 2 != 0 and direction == 'both') or direction == 'right':\n",
    "      print('>> right side generation')\n",
    "      candidates = candidates_gen(sent_tokens=sent_tokens, d='right', n_candidates=first_sample_size, topk=topk, temperature=temperature)\n",
    "      beams, sorted_beams = beam_gen(sent_tokens, candidates, n_tokens-1, 'right', sample_size, topk, temperature=temperature)\n",
    "      topn = len(sorted_beams)//5 if len(sorted_beams) > 4 else len(sorted_beams)\n",
    "      selected_candidate = random.choice(sorted_beams[:topn])\n",
    "      sent_tokens = sent_tokens + selected_candidate[0]\n",
    "      print(tokenizer.decode(sent_tokens))\n",
    "    \n",
    "  return tokenizer.decode(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66926791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> left side generation\n",
      "Number of beams: 8\n",
      "Number of beams: 16\n",
      "Number of beams: 32\n",
      ".pg) python is a\n",
      ">> right side generation\n",
      "Number of beams: 8\n",
      "Number of beams: 16\n",
      "Number of beams: 32\n",
      ".pg) python is a Java script generator.\n",
      ">> left side generation\n",
      "Number of beams: 8\n",
      "Number of beams: 16\n",
      "Number of beams: 32\n",
      "ttm.pg) python is a Java script generator.\n",
      ">> right side generation\n",
      "Number of beams: 8\n",
      "Number of beams: 16\n",
      "Number of beams: 32\n",
      "ttm.pg) python is a Java script generator. One of many Python\n",
      ">> left side generation\n",
      "Number of beams: 8\n",
      "Number of beams: 16\n",
      "Number of beams: 32\n",
      ".bg) ttm.pg) python is a Java script generator. One of many Python\n",
      ">> right side generation\n",
      "Number of beams: 8\n",
      "Number of beams: 16\n",
      "Number of beams: 32\n",
      ".bg) ttm.pg) python is a Java script generator. One of many Python languages. One of\n"
     ]
    }
   ],
   "source": [
    "sent = \"python is a \"  \n",
    "first_sample_size = 4\n",
    "sample_size = 2\n",
    "n_tokens = 4\n",
    "topk = 20\n",
    "iterations = 6\n",
    "temperature = 4\n",
    "direction = \"both\"\n",
    "\n",
    "bi_generator(sent, direction, first_sample_size, sample_size, n_tokens, topk, iterations, temperature);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34619972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLNetTokenizer, XLNetModel\n",
    "import torch\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "model = XLNetModel.from_pretrained('xlnet-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9000a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'a', 'h', 'd'\n",
    "sent_tokens = tokenizer.encode(sent)\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda8885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c9572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
